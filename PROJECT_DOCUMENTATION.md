# Emotion-Aware Multi-Factor Authentication System
## Complete Technical Documentation

---

## Table of Contents
1. [Project Overview](#1-project-overview)
2. [System Architecture](#2-system-architecture)
3. [Datasets Used](#3-datasets-used)
4. [Machine Learning Models](#4-machine-learning-models)
5. [Authentication Logic & Formulas](#5-authentication-logic--formulas)
6. [Frontend Technologies](#6-frontend-technologies)
7. [Backend Technologies](#7-backend-technologies)
8. [Security Features](#8-security-features)
9. [API Endpoints](#9-api-endpoints)
10. [Deployment](#10-deployment)

---

## 1. Project Overview

### 1.1 Purpose
An advanced multi-factor authentication system that combines traditional credentials with biometric and behavioral analysis, including real-time emotion detection to enhance security and detect potential coercion scenarios.

### 1.2 Key Features
- **5-Layer Authentication**: Passphrase, Face Detection, Facial Emotion, Voice Emotion, Keystroke Dynamics
- **Real-Time Emotion Analysis**: Continuous monitoring every 3 seconds
- **Coercion Detection**: Identifies potential duress situations
- **Admin Dashboard**: Comprehensive monitoring and alert management
- **Visual Feedback**: Live curvature detection overlays on video feed
- **Security Alerts**: Multi-level alert system (Critical, High, Medium, Low)

### 1.3 Use Cases
- High-security environments requiring emotion-aware authentication
- Banking and financial systems
- Healthcare systems with patient data access
- Government and military applications
- Corporate security systems

---

## 2. System Architecture

### 2.1 High-Level Architecture
```
Frontend (HTML/JS) â†” REST API â†” Backend (FastAPI) â†” ML Models â†” Database
```

### 2.2 Component Breakdown

**Frontend Layer:**
- Authentication Interface (index.html)
- Admin Dashboard (admin.html)
- Real-time video processing with Canvas API
- Audio analysis with Web Audio API

**Backend Layer:**
- FastAPI REST API (main.py)
- Fusion Engine (fusion_engine.py)
- Alert System (alert_system.py)
- Database Management (db.py)

**ML/Analysis Layer:**
- Facial Emotion Recognition (CNN + Heuristic)
- Voice Emotion Recognition (Random Forest + Heuristic)
- Keystroke Dynamics (Statistical Analysis)
- Environmental Context Analysis

**Storage Layer:**
- SQLite Database (users, authentication logs)
- JSON Files (alerts, trained models)
- File System (model weights, datasets)



---

## 3. Datasets Used

### 3.1 Facial Emotion Recognition

**Primary Dataset: FER2013**
- **Source**: Kaggle (https://www.kaggle.com/datasets/msambare/fer2013)
- **Size**: 35,887 grayscale images (48x48 pixels)
- **Classes**: 7 emotions (angry, disgust, fear, happy, sad, surprise, neutral)
- **Split**: Training (28,709), PublicTest (3,589), PrivateTest (3,589)
- **Format**: CSV file with pixel values

**Alternative Datasets:**
- CK+ (Extended Cohn-Kanade)
- AffectNet
- RAF-DB (Real-world Affective Faces Database)

### 3.2 Voice Emotion Recognition

**Primary Dataset: RAVDESS**
- **Source**: Zenodo/Kaggle (https://zenodo.org/record/1188976)
- **Size**: 7,356 audio files (24GB)
- **Speakers**: 24 professional actors (12 male, 12 female)
- **Emotions**: 8 emotions (neutral, calm, happy, sad, angry, fearful, disgust, surprised)
- **Format**: WAV audio files (16-bit, 48kHz)

**Alternative Datasets:**
- TESS (Toronto Emotional Speech Set)
- CREMA-D (Crowd-sourced Emotional Multimodal Actors Dataset)
- EMO-DB (Berlin Database of Emotional Speech)

### 3.3 Keystroke Dynamics

**Primary Dataset: CMU Keystroke Dynamics**
- **Source**: Carnegie Mellon University
- **Type**: Timing patterns of keystrokes
- **Features**: Key press duration, flight time between keys
- **Format**: JSON files with keystroke events

**Synthetic Generation:**
- System generates synthetic keystroke data for testing
- User-specific patterns learned during enrollment
- 50 samples per user with realistic timing variations

### 3.4 Dataset Preparation

**Download Script:** `backend/download_datasets.py`
```bash
# Download sample datasets
python -m backend.download_datasets --samples

# Download FER2013 (requires Kaggle API)
python -m backend.download_datasets --fer2013

# Download RAVDESS (manual download required)
python -m backend.download_datasets --ravdess
```



---

## 4. Machine Learning Models

### 4.1 Facial Emotion Recognition Model

**Architecture: Convolutional Neural Network (CNN)**

```python
Model: Sequential
â”œâ”€â”€ Conv2D(64, 3x3) + BatchNorm + MaxPool + Dropout(0.25)
â”œâ”€â”€ Conv2D(128, 3x3) + BatchNorm + MaxPool + Dropout(0.25)
â”œâ”€â”€ Conv2D(256, 3x3) + BatchNorm + MaxPool + Dropout(0.25)
â”œâ”€â”€ Flatten
â”œâ”€â”€ Dense(512) + BatchNorm + Dropout(0.5)
â””â”€â”€ Dense(7, softmax) â†’ 7 emotion classes
```

**Training Details:**
- **Optimizer**: Adam (lr=0.0001)
- **Loss**: Categorical Crossentropy
- **Epochs**: 50 with early stopping
- **Batch Size**: 64
- **Data Augmentation**: Rotation, shift, zoom, flip

**Heuristic Fallback (Primary Method):**
- **Method**: Lip and Eye Curvature Analysis
- **Technique**: Canny Edge Detection + Geometric Analysis
- **Features**:
  - Mouth curvature (upward = smile, downward = frown)
  - Smile ratio (horizontal vs vertical edges)
  - Eye openness (edge density in eye region)
  - Eye shape analysis (wide vs squinted)

**Emotion Formulas (Heuristic):**
```python
# Happy: Upward mouth + horizontal edges + squinted eyes
happy = 0.60 * max(0, mouth_curvature * 12) + 
        0.25 * min(1.0, smile_ratio / 1.8) + 
        0.15 * (0.5 - abs(eye_openness - 0.4))

# Sad: Downward mouth + droopy eyes
sad = 0.60 * max(0, -mouth_curvature * 12) + 
      0.25 * (1.0 - min(1.0, smile_ratio / 1.8)) + 
      0.15 * (0.3 - abs(eye_openness - 0.3))

# Surprise: Wide eyes + high edge density
surprise = 0.60 * eye_openness + 
           0.30 * eye_edge_density + 
           0.10 * (1.0 - abs(mouth_curvature))

# Angry: Tense features + slight frown
angry = 0.40 * eye_edge_density + 
        0.35 * max(0, -mouth_curvature * 8) + 
        0.25 * (0.5 - abs(eye_openness - 0.5))
```

**Model File:** `backend/storage/trained_models/facial_emotion_cnn.h5`

### 4.2 Voice Emotion Recognition Model

**Architecture: Random Forest Classifier**

```python
Model: RandomForestClassifier
â”œâ”€â”€ n_estimators: 200
â”œâ”€â”€ max_depth: 20
â”œâ”€â”€ min_samples_split: 5
â””â”€â”€ Features: RMS, ZCR, Pitch, MFCCs (13), Spectral features
```

**Feature Extraction:**
- **RMS (Root Mean Square)**: Energy/volume level
- **ZCR (Zero Crossing Rate)**: Voice quality/roughness
- **Pitch**: Fundamental frequency (80-500 Hz)
- **MFCCs**: Mel-frequency cepstral coefficients (13 coefficients)
- **Spectral Centroid**: Brightness of sound
- **Spectral Rolloff**: Shape of signal

**Heuristic Fallback (Primary Method):**
```python
# Normalize pitch to 0-1 range (80-300 Hz typical speech)
norm_pitch = (pitch - 80) / 220

# Happy: Moderate-high energy + higher pitch + smooth voice
happy = 0.4 * rms + 0.3 * (1.0 - abs(norm_pitch - 0.6)) + 0.3 * (1.0 - zcr)

# Calm: Low energy + low ZCR + lower pitch
calm = 0.4 * (1.0 - rms) + 0.3 * (1.0 - zcr) + 0.3 * (1.0 - abs(norm_pitch - 0.3))

# Angry: High energy + high ZCR (rough voice)
angry = 0.5 * rms + 0.4 * zcr + 0.1 * norm_pitch

# Sad: Low energy + low pitch + smooth voice
sad = 0.4 * (1.0 - rms) + 0.3 * (1.0 - norm_pitch) + 0.3 * (1.0 - zcr)
```

**Pitch Detection:**
- **Method**: Autocorrelation algorithm
- **Sample Rate**: 48kHz (Web Audio API)
- **Valid Range**: 80-500 Hz (human voice)
- **Energy Threshold**: RMS > 0.01 for valid detection

**Model Files:**
- `backend/storage/trained_models/voice_emotion_rf.pkl`
- `backend/storage/trained_models/voice_emotion_scaler.pkl`



### 4.3 Keystroke Dynamics Model

**Method: Statistical Analysis + Isolation Forest**

**Features Extracted:**
- **Hold Time**: Duration of key press (t_up - t_down)
- **Flight Time**: Time between consecutive key releases
- **Digraph Latency**: Time between specific key pairs
- **Typing Speed**: Overall words per minute
- **Rhythm Patterns**: Variance in timing

**Enrollment Process:**
- User types passphrase 3-5 times
- System extracts timing features
- Builds statistical profile (mean, std, percentiles)
- Trains Isolation Forest for anomaly detection

**Authentication Scoring:**
```python
# Calculate match score
match_score = 1.0 - (distance / max_distance)

# Anomaly detection
anomaly_score = isolation_forest.decision_function(features)

# Confidence calculation
confidence = (match_score + (1 - anomaly_score)) / 2
```

**Model Storage:** `backend/storage/kd_models/{user_id}_kd_model.pkl`

### 4.4 Model Training

**Training Script:** `backend/train_models.py`

```bash
# Train all models
python -m backend.train_models --all

# Train specific models
python -m backend.train_models --facial
python -m backend.train_models --voice
python -m backend.train_models --keystroke
```

**Training Time:**
- Facial CNN: ~2-4 hours (50 epochs, GPU recommended)
- Voice RF: ~30-60 minutes (CPU sufficient)
- Keystroke: Real-time during enrollment

**Model Evaluation:** `backend/evaluate_models.py`
- Accuracy, Precision, Recall, F1-Score
- Confusion Matrix
- ROC Curves



---

## 5. Authentication Logic & Formulas

### 5.1 Multi-Layer Authentication Flow

```
Layer 1: Passphrase Validation
    â†“ (Pass)
Layer 2: Face Count Detection (Exactly 1 face)
    â†“ (Pass)
Layer 3: Facial Emotion Check (Positive/Neutral)
    â†“ (Pass)
Layer 4: Voice Emotion Check (Positive/Neutral)
    â†“ (Pass)
Layer 5: Keystroke Dynamics (Pattern Match)
    â†“ (Pass)
DECISION: PERMIT / DELAY / DENY
```

### 5.2 Emotion-Based Access Control

**Positive/Neutral Emotions (ALLOW):**
- Facial: happy, surprised, neutral
- Voice: happy, calm, surprised, neutral

**Negative Emotions (DENY):**
- Facial: sad, angry, fear, disgust
- Voice: sad, angry, fearful, disgust

**Logic:**
```python
facial_is_positive = dominant_facial in ["happy", "surprised", "neutral"]
voice_is_positive = dominant_voice in ["happy", "calm", "surprised", "neutral"]

if not facial_is_positive or not voice_is_positive:
    decision = "deny"
    # Both emotions must be positive for access
```

### 5.3 Fusion Engine Decision Logic

**File:** `backend/fusion_engine.py`

**Stress Calculation:**
```python
# Weighted stress from facial and voice
stress_facial = (angry * 0.8 + fear * 0.7 + disgust * 0.6 + sad * 0.3)
stress_voice = (angry * 0.9 + fearful * 0.8 + sad * 0.4 + disgust * 0.6)

# Combined stress (reduced by 30% for less sensitivity)
stress = (0.5 * stress_facial + 0.5 * stress_voice) * 0.7
```

**Biometric Score:**
```python
biometric_score = (
    (1.0 - stress) * 0.4 +      # Lower stress is better
    match * 0.4 +                # Keystroke match
    (1.0 - anomaly) * 0.2        # Lower anomaly is better
)
```

**Environmental Score:**
```python
env_score = env_stability * 0.6 + env_quality * 0.4
```

**Authentication Score:**
```python
auth_score = biometric_score * 0.7 + env_score * 0.3
```

**Decision Criteria:**

**DENY (Immediate):**
- Incorrect passphrase
- Multiple people detected (>1 face)
- No face detected (0 faces)
- Negative facial emotion
- Negative voice emotion
- Coercion risk > 0.85
- Keystroke anomaly > 0.85
- Shouting detected with stress > 0.8

**DELAY (Suspicious):**
- Stress > 0.9
- Coercion risk > 0.7
- Keystroke match < 0.15
- Poor environment with high stress

**PERMIT (Success):**
- All checks passed
- Both emotions positive/neutral
- Reasonable keystroke match
- Low stress levels

### 5.4 Confidence & Stress Adjustment

**Based on Decision Type:**

```python
if decision == "permit":
    confidence = 0.75 to 0.95  # High confidence
    stress = stress * 0.4       # Reduce to 0.0-0.3 range

elif decision == "delay":
    confidence = 0.45 to 0.65  # Medium confidence
    stress = stress * 0.6       # Reduce to 0.3-0.6 range

elif decision == "deny":
    confidence = 0.15 to 0.45  # Low confidence
    stress = stress * 0.8       # Keep in 0.5-0.9 range
```



---

## 6. Frontend Technologies

### 6.1 Core Technologies

**HTML5:**
- Semantic markup
- Canvas API for video overlay
- Video element for camera feed
- Form elements for user input

**CSS3:**
- Custom properties (CSS variables) for theming
- Flexbox and Grid layouts
- Animations and transitions
- Responsive design with media queries

**JavaScript (Vanilla ES6+):**
- Async/await for API calls
- Web Audio API for voice analysis
- MediaDevices API for camera access
- Canvas 2D API for visualization

### 6.2 Key Frontend Features

**Real-Time Video Processing:**
```javascript
// Camera initialization
navigator.mediaDevices.getUserMedia({video: true, audio: true})

// Auto-capture every 3 seconds
setInterval(() => captureFrame(), 3000)

// Canvas overlay for visualization
overlayCanvas.getContext('2d').drawImage(overlayImage, 0, 0)
```

**Audio Analysis:**
```javascript
// Web Audio API setup
const audioContext = new AudioContext()
const analyser = audioContext.createAnalyser()
analyser.fftSize = 2048

// Pitch detection using autocorrelation
function detectPitch(buffer) {
    // Autocorrelation algorithm
    // Returns frequency in Hz
}

// Real-time feature extraction
voiceFeatures = {
    rms: Math.sqrt(sum / length),
    zcr: crossings / length,
    pitch_hz: detectPitch(buffer)
}
```

**Keystroke Tracking:**
```javascript
// Track key press/release timing
input.addEventListener('keydown', e => {
    events.push({
        key: e.key,
        code: e.code,
        t_down: performance.now()
    })
})

input.addEventListener('keyup', e => {
    events.push({
        key: e.key,
        code: e.code,
        t_up: performance.now()
    })
})
```

### 6.3 User Interface Components

**Authentication Page (index.html):**
- User Identity Card (User ID, Passphrase)
- Live Camera Feed with Overlay
- Biometric Data Display (RMS, ZCR, Pitch)
- Emotion Display (Facial, Voice, Stress)
- Enrollment Section (Side-by-side)
- Authentication Section

**Admin Dashboard (admin.html):**
- Statistics Cards (Attempts, Success Rate, Alerts, Users)
- Authentication Logs Table
- Security Alerts Section with Pending Area
- Detailed Statistics Tab
- Real-time Updates (30-second refresh)

**Theme:**
- Professional black and golden gradient
- Color scheme: #d4af37 (gold), #000000 (black)
- Smooth transitions and hover effects
- Responsive design for mobile/tablet

### 6.4 Visualization Features

**Curvature Detection Overlay:**
- Cyan lines: Mouth/lip edges
- Yellow lines: Eye edges
- Green box: Eye region
- Magenta box: Mouth region
- Curve indicators: Smile/frown/neutral with values
- Eye indicators: Wide/squint with values
- Legend: Color coding in top-right

**Real-Time Updates:**
- Emotion values update every 3 seconds
- Mic status: "Speaking" or "Listening"
- Camera status: Face count warnings
- Processing lock to prevent overlapping requests



---

## 7. Backend Technologies

### 7.1 Core Technologies

**Python 3.10+**
- Modern async/await support
- Type hints for better code quality
- Dataclasses for structured data

**FastAPI Framework:**
- High-performance async web framework
- Automatic API documentation (Swagger/OpenAPI)
- Pydantic models for request/response validation
- CORS middleware for cross-origin requests

**SQLAlchemy ORM:**
- Database abstraction layer
- SQLite for development/production
- Declarative models
- Session management

**Machine Learning Libraries:**
- TensorFlow/Keras: Deep learning (CNN)
- Scikit-learn: Random Forest, Isolation Forest
- NumPy: Numerical computations
- OpenCV: Computer vision (edge detection, face detection)
- Librosa: Audio feature extraction (optional)

### 7.2 Backend Architecture

**Main API Server (main.py):**
```python
app = FastAPI(title="Emotion-Aware Contextual Auth")

# Key endpoints
POST /api/enroll/keystrokes      # Enroll user typing pattern
POST /api/auth/attempt           # Authenticate user
POST /api/analyze/emotion        # Real-time emotion analysis
GET  /api/admin/logs             # Get authentication logs
GET  /api/admin/alerts           # Get security alerts
GET  /api/admin/statistics       # Get system statistics
```

**Database Models (db.py):**
```python
class UserProfile:
    - user_id: String (primary key)
    - passphrase: String (hashed)
    - kd_model_path: String (keystroke model path)
    - kd_enroll_count: Integer
    - created_at: DateTime

class EventLog:
    - id: Integer (primary key)
    - user_id: String
    - decision: String (permit/delay/deny)
    - confidence: Float
    - reason: String
    - facial: JSON (emotion data)
    - voice: JSON (emotion data)
    - keystroke: JSON (dynamics data)
    - env: JSON (environmental data)
    - fusion: JSON (decision data)
    - created_at: DateTime
```

**Alert System (alert_system.py):**
```python
class AlertSystem:
    - alerts: List[Dict]
    - alert_levels: Dict (critical, high, medium, low)
    
    Methods:
    - create_alert()
    - get_recent_alerts()
    - get_critical_alerts()
    - acknowledge_alert()
    - resolve_alert()
    - get_alert_statistics()
```

**Fusion Engine (fusion_engine.py):**
```python
def fuse(facial, voice, keystroke, env) -> Decision:
    # Combines all modalities
    # Returns: decision, confidence, stress, guidance
```

### 7.3 Model Modules

**Facial Emotion (facial_emotion_trained.py):**
- CNN model loading
- Heuristic curvature analysis
- Face detection (Haar Cascade)
- Visualization generation
- Returns: emotion probabilities, stress, dominant emotion

**Voice Emotion (voice_emotion_trained.py):**
- Random Forest model loading
- Heuristic feature-based analysis
- Returns: emotion probabilities, stress, dominant emotion

**Keystroke Dynamics (keystroke_dynamics.py):**
- Statistical profile building
- Isolation Forest anomaly detection
- Match score calculation
- Returns: match score, anomaly score, confidence

**Environmental Context (env_context.py):**
- Brightness analysis
- Noise level detection
- Environmental suitability scoring
- Returns: quality score, stability, recommendations

### 7.4 Dependencies

**requirements.txt:**
```
fastapi==0.104.1
uvicorn==0.24.0
sqlalchemy==2.0.23
pydantic==2.5.0
numpy==1.24.3
opencv-python==4.8.1.78
tensorflow==2.14.0
scikit-learn==1.3.2
pandas==2.1.3
python-multipart==0.0.6
```



---

## 8. Security Features

### 8.1 Multi-Layer Security

**Layer 1: Passphrase Validation**
- Triple validation (User Identity, Enrollment, Authentication)
- Frontend and backend validation
- Immediate denial on mismatch
- High-priority alert creation

**Layer 2: Face Count Detection**
- Haar Cascade face detection
- Exactly 1 face required
- Critical alert for multiple people (coercion detection)
- Warning for no face detected

**Layer 3: Facial Emotion Verification**
- Curvature-based emotion detection
- Only positive/neutral emotions allowed
- Denies: sad, angry, fear, disgust
- Real-time visual feedback

**Layer 4: Voice Emotion Verification**
- Pitch, energy, and quality analysis
- Only positive/neutral emotions allowed
- Denies: sad, angry, fearful, disgust
- Continuous monitoring

**Layer 5: Keystroke Dynamics**
- Behavioral biometric verification
- User-specific typing patterns
- Anomaly detection
- Enrollment required (3-5 samples)

### 8.2 Alert System

**Alert Levels:**

ðŸ”´ **CRITICAL** (Immediate Response Required):
- Multiple people detected
- High coercion risk (>85%)
- Shouting detected

ðŸŸ  **HIGH** (Security Concern):
- Incorrect passphrase
- Keystroke anomaly (>70%)
- Authentication denied

ðŸŸ¡ **MEDIUM** (User Wellness):
- High stress levels (>70%)
- Mental health alert

ðŸŸ¢ **LOW** (Environmental):
- Unsuitable environment
- Poor lighting/noise

**Alert Management:**
- Pending alerts section (requires acknowledgment)
- Acknowledge individual or all pending
- Full alert history
- Detailed information with expandable views

### 8.3 Coercion Detection

**Indicators:**
- Multiple people in frame
- Shouting or very loud environment
- High stress with environmental anomalies
- Negative emotions (fear, disgust)
- Voice tremor with high pitch

**Response:**
- Immediate access denial
- Critical alert creation
- Detailed logging
- Admin notification

### 8.4 Data Security

**Passphrase Storage:**
- Stored in database (should be hashed in production)
- Validated on every authentication attempt
- Never exposed in logs or API responses

**Biometric Data:**
- Processed in real-time
- Not permanently stored (privacy-preserving)
- Only statistical features stored for keystroke

**Session Management:**
- Stateless authentication
- No session cookies
- Each request independently validated

**Database Security:**
- SQLite with file permissions
- Prepared statements (SQL injection prevention)
- Input validation with Pydantic models



---

## 9. API Endpoints

### 9.1 Authentication Endpoints

**POST /api/enroll/keystrokes**
```json
Request:
{
  "user_id": "string",
  "passphrase": "string",
  "samples": [[keystroke_events]]
}

Response:
{
  "ok": true,
  "kd_model_path": "string",
  "enrolled_samples": 3
}
```

**POST /api/auth/attempt**
```json
Request:
{
  "user_id": "string",
  "passphrase": "string",
  "frame_data_url": "data:image/png;base64,...",
  "voice_features": {"rms": 0.5, "zcr": 0.3, "pitch_hz": 180},
  "keystroke_events": [...]
}

Response:
{
  "ok": true,
  "fusion": {
    "decision": "permit|delay|deny",
    "confidence": 0.87,
    "stress": 0.18,
    "reason": "string",
    "guidance": "string",
    "facial_emotion": "happy",
    "voice_emotion": "calm",
    "emotion_check_passed": true
  },
  "facial": {...},
  "voice": {...},
  "keystroke": {...},
  "env": {...}
}
```

**POST /api/analyze/emotion**
```json
Request:
{
  "frame_data_url": "data:image/png;base64,...",
  "voice_features": {"rms": 0.5, "zcr": 0.3, "pitch_hz": 180}
}

Response:
{
  "ok": true,
  "facial": {
    "dominant_emotion": "happy",
    "stress": 0.15,
    "probs": {...},
    "visualization": {
      "overlay_image": "data:image/jpeg;base64,..."
    }
  },
  "voice": {
    "dominant_emotion": "calm",
    "stress": 0.12,
    "probs": {...}
  }
}
```

### 9.2 Admin Endpoints

**GET /api/admin/logs?limit=50**
```json
Response:
{
  "ok": true,
  "logs": [
    {
      "id": 1,
      "user_id": "user123",
      "decision": "permit",
      "confidence": 0.87,
      "reason": "All checks passed",
      "created_at": "2024-11-04T20:00:00",
      "facial": {...},
      "voice": {...},
      "keystroke": {...},
      "fusion": {...}
    }
  ]
}
```

**GET /api/admin/alerts?limit=50&level=critical**
```json
Response:
{
  "ok": true,
  "alerts": [
    {
      "id": 1,
      "type": "multiple_people_detected",
      "level": "critical",
      "user_id": "user123",
      "message": "SECURITY BREACH: Multiple people detected",
      "details": {"num_faces": 2},
      "timestamp": "2024-11-04T20:00:00",
      "acknowledged": false,
      "resolved": false
    }
  ]
}
```

**GET /api/admin/statistics**
```json
Response:
{
  "ok": true,
  "authentication": {
    "total_attempts": 150,
    "permits": 120,
    "delays": 15,
    "denies": 15,
    "success_rate": 80.0
  },
  "alerts": {
    "total_alerts": 25,
    "by_level": {
      "critical": 3,
      "high": 8,
      "medium": 10,
      "low": 4
    },
    "unacknowledged": 5
  },
  "users": {
    "total": 10,
    "enrolled": 8
  }
}
```

**POST /api/admin/alerts/{alert_id}/acknowledge**
```json
Response:
{
  "ok": true,
  "message": "Alert acknowledged"
}
```



---

## 10. Deployment

### 10.1 Installation

**Prerequisites:**
- Python 3.10 or higher
- pip package manager
- Webcam and microphone
- Modern web browser (Chrome, Firefox, Edge)

**Step 1: Clone Repository**
```bash
git clone <repository-url>
cd emotion_auth_system
```

**Step 2: Install Dependencies**
```bash
pip install -r requirements.txt
```

**Step 3: Download Datasets (Optional)**
```bash
# Create sample datasets for testing
python -m backend.download_datasets --samples

# Or download real datasets
python -m backend.download_datasets --fer2013
python -m backend.download_datasets --ravdess
```

**Step 4: Train Models (Optional)**
```bash
# Train all models (requires datasets)
python -m backend.train_models --all

# Or use pre-trained models / heuristic fallback
# System works without training using heuristic methods
```

**Step 5: Start Server**
```bash
# Windows
start_server.bat

# Linux/Mac
uvicorn backend.main:app --reload

# Or using Python
python -m uvicorn backend.main:app --reload
```

**Step 6: Access Application**
- Authentication: http://localhost:8000/web/index.html
- Admin Dashboard: http://localhost:8000/web/admin.html
- API Docs: http://localhost:8000/docs

### 10.2 Configuration

**Environment Variables:**
```bash
# Server configuration
HOST=0.0.0.0
PORT=8000

# Database
DATABASE_URL=sqlite:///./backend/storage/auth.db

# Model paths
MODELS_DIR=./backend/storage/trained_models
```

**Database Initialization:**
```python
# Automatic on first run
# Creates: backend/storage/auth.db
# Tables: user_profiles, event_logs
```

### 10.3 Production Deployment

**Security Enhancements:**
1. Enable HTTPS/TLS
2. Hash passphrases (bcrypt/argon2)
3. Add rate limiting
4. Implement session management
5. Use PostgreSQL instead of SQLite
6. Add authentication for admin endpoints
7. Enable CORS restrictions
8. Add input sanitization
9. Implement audit logging
10. Use environment variables for secrets

**Performance Optimization:**
1. Use GPU for CNN inference
2. Implement caching for models
3. Add request queuing
4. Use CDN for static files
5. Enable compression
6. Optimize image sizes
7. Implement connection pooling
8. Add load balancing

**Monitoring:**
1. Set up logging (structured logs)
2. Add metrics collection (Prometheus)
3. Implement health checks
4. Monitor alert rates
5. Track authentication success rates
6. Monitor model performance
7. Set up alerting (email/SMS)

### 10.4 Docker Deployment (Optional)

**Dockerfile:**
```dockerfile
FROM python:3.10-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8000
CMD ["uvicorn", "backend.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Docker Compose:**
```yaml
version: '3.8'
services:
  app:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./backend/storage:/app/backend/storage
    environment:
      - DATABASE_URL=sqlite:///./backend/storage/auth.db
```

**Run:**
```bash
docker-compose up -d
```



---

## 11. Usage Guide

### 11.1 User Enrollment

**Step 1: Enter User Information**
1. Open http://localhost:8000/web/index.html
2. Enter User ID in "User Identity" section
3. Enter Passphrase (remember this!)
4. Check "I consent to biometric data capture"

**Step 2: Enroll Keystroke Pattern**
1. In the "Step 1: Enroll" section (left side)
2. Type the SAME passphrase 3-5 times
3. Click "âœ“ Submit Sample" after each typing
4. Wait for confirmation: "Sample X saved!"
5. Repeat until you have 3-5 samples

**Step 3: Ready to Authenticate**
- Once enrolled, you can authenticate using the right side

### 11.2 User Authentication

**Step 1: Enter Credentials**
1. Enter User ID and Passphrase in "User Identity" section
2. Ensure camera and microphone are enabled

**Step 2: Authenticate**
1. In the "Step 2: Authenticate" section (right side)
2. Type the SAME passphrase
3. Click "ðŸ”“ Authenticate Now"
4. System will analyze:
   - Face detection (1 person only)
   - Facial emotion (must be positive/neutral)
   - Voice emotion (must be positive/neutral)
   - Keystroke pattern (must match enrolled)

**Step 3: View Result**
- **PERMIT**: Access granted (green)
- **DELAY**: Suspicious activity (yellow)
- **DENY**: Access denied (red)

### 11.3 Real-Time Monitoring

**Biometric Data Display:**
- **RMS**: Voice energy level (0.00-1.00)
- **ZCR**: Voice quality (0.00-1.00)
- **Pitch**: Voice frequency (Hz)
- **Facial Emotion**: Detected emotion with curvature lines
- **Voice Emotion**: Detected emotion from audio
- **Stress Levels**: Facial and voice stress percentages

**Visual Overlay:**
- Cyan lines: Mouth/lip edges
- Yellow lines: Eye edges
- Green box: Eye detection region
- Magenta box: Mouth detection region
- Arrows/curves: Smile/frown indicators

### 11.4 Admin Dashboard

**Access:** http://localhost:8000/web/admin.html

**Statistics Cards:**
- Total authentication attempts
- Success rate with progress bar
- Active (unacknowledged) alerts
- Enrolled users count

**Authentication Logs Tab:**
- View all authentication attempts
- Filter by decision (permit/delay/deny)
- See detailed biometric data
- Export logs (future feature)

**Security Alerts Tab:**
- **Pending Alerts**: Requires immediate attention
  - Acknowledge individual alerts
  - Acknowledge all pending at once
- **All Alerts History**: Complete alert record
- Expandable details for each alert
- Color-coded by severity

**Statistics Tab:**
- Authentication breakdown (permits, delays, denies)
- Alert breakdown by level
- User statistics
- Pending alerts count

### 11.5 Testing Scenarios

**Successful Authentication:**
1. Smile or stay neutral
2. Speak calmly
3. Type normally
4. Result: PERMIT

**Failed Authentication (Negative Emotion):**
1. Frown or look sad
2. Result: DENY (negative facial emotion)

**Failed Authentication (Multiple People):**
1. Have someone else in frame
2. Result: DENY (security breach)

**Failed Authentication (Wrong Passphrase):**
1. Type incorrect passphrase
2. Result: DENY (authentication failure)



---

## 12. Performance Metrics

### 12.1 Model Accuracy

**Facial Emotion Recognition:**
- CNN Model: ~65-70% accuracy on FER2013
- Heuristic Method: ~60-65% accuracy (more consistent)
- Real-time processing: ~30-50ms per frame

**Voice Emotion Recognition:**
- Random Forest: ~70-75% accuracy on RAVDESS
- Heuristic Method: ~65-70% accuracy
- Real-time processing: ~10-20ms per analysis

**Keystroke Dynamics:**
- False Accept Rate (FAR): <5%
- False Reject Rate (FRR): <10%
- Equal Error Rate (EER): ~7-8%

### 12.2 System Performance

**Response Times:**
- Real-time emotion analysis: ~100-200ms
- Full authentication: ~500-800ms
- Admin dashboard load: ~200-400ms

**Resource Usage:**
- CPU: 10-30% (during active authentication)
- Memory: 200-500MB
- Storage: ~100MB (models + database)

**Scalability:**
- Concurrent users: 10-50 (single server)
- Database: SQLite (suitable for <100 users)
- Upgrade to PostgreSQL for production

### 12.3 Browser Compatibility

**Supported Browsers:**
- âœ… Chrome 90+ (Recommended)
- âœ… Firefox 88+
- âœ… Edge 90+
- âœ… Safari 14+ (limited testing)
- âŒ Internet Explorer (not supported)

**Required Features:**
- WebRTC (getUserMedia)
- Web Audio API
- Canvas API
- ES6+ JavaScript
- Fetch API

---

## 13. Troubleshooting

### 13.1 Common Issues

**Camera Not Working:**
- Check browser permissions
- Ensure HTTPS or localhost
- Try different browser
- Check if camera is in use by another app

**Microphone Not Detecting:**
- Check browser permissions
- Verify microphone is not muted
- Test with system sound settings
- Try speaking louder

**Models Not Loading:**
- Check if model files exist in `backend/storage/trained_models/`
- System will use heuristic fallback automatically
- Train models using `python -m backend.train_models --all`

**Database Errors:**
- Delete `backend/storage/auth.db` to reset
- Check file permissions
- Ensure SQLite is installed

**High CPU Usage:**
- Reduce capture frequency (change from 3s to 5s)
- Disable visualization overlay
- Use smaller image sizes
- Close other applications

### 13.2 Debug Mode

**Enable Debug Logging:**
```python
# In backend/main.py
import logging
logging.basicConfig(level=logging.DEBUG)
```

**Browser Console:**
- Press F12 to open developer tools
- Check Console tab for JavaScript errors
- Check Network tab for API errors

**Server Logs:**
- Check terminal/console output
- Look for DEBUG messages
- Check for error stack traces

---

## 14. Future Enhancements

### 14.1 Planned Features

**Security:**
- Multi-device authentication
- Biometric template protection
- Blockchain-based audit logs
- Advanced anti-spoofing (liveness detection)

**Machine Learning:**
- Transformer-based emotion recognition
- Continuous learning from user feedback
- Personalized emotion baselines
- Context-aware authentication

**User Experience:**
- Mobile app (iOS/Android)
- Progressive Web App (PWA)
- Voice commands
- Accessibility improvements

**Integration:**
- OAuth 2.0 / OpenID Connect
- SAML support
- LDAP/Active Directory
- REST API for third-party apps

**Analytics:**
- Advanced reporting
- Predictive analytics
- User behavior patterns
- Anomaly detection improvements

### 14.2 Research Opportunities

- Multimodal fusion techniques
- Explainable AI for decisions
- Privacy-preserving biometrics
- Federated learning
- Edge computing deployment

---

## 15. References

### 15.1 Datasets

1. FER2013: https://www.kaggle.com/datasets/msambare/fer2013
2. RAVDESS: https://zenodo.org/record/1188976
3. CMU Keystroke: http://www.cs.cmu.edu/~keystroke/

### 15.2 Research Papers

1. "Facial Emotion Recognition Using Convolutional Neural Networks"
2. "Speech Emotion Recognition: A Review"
3. "Keystroke Dynamics for User Authentication"
4. "Multimodal Biometric Authentication Systems"

### 15.3 Technologies

1. FastAPI: https://fastapi.tiangolo.com/
2. TensorFlow: https://www.tensorflow.org/
3. OpenCV: https://opencv.org/
4. Web Audio API: https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API

---

## 16. License & Credits

### 16.1 License
This project is for educational and research purposes.

### 16.2 Credits
- Emotion detection algorithms based on academic research
- Datasets provided by respective institutions
- Open-source libraries and frameworks

---

## 17. Contact & Support

For questions, issues, or contributions:
- Check documentation: README.md, AUTHENTICATION_FLOW.md
- Review code comments
- Test with sample data first
- Enable debug logging for troubleshooting

---

**Document Version:** 1.0  
**Last Updated:** November 4, 2024  
**Project Status:** Active Development

